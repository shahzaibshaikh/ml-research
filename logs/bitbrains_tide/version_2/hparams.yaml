MaxPool1d: true
activation: ReLU
batch_norm: false
dropout: 0.1
input_chunk_length: 512
input_dim: 1
layer_widths:
- 512
- 512
- 512
likelihood: null
lr_scheduler_cls: !!python/name:torch.optim.lr_scheduler.ExponentialLR ''
lr_scheduler_kwargs:
  gamma: 0.999
n_freq_downsample: !!python/tuple
- !!python/tuple
  - 128
- !!python/tuple
  - 11
- !!python/tuple
  - 1
nr_params: 1
num_blocks: 1
num_layers: 2
num_stacks: 3
optimizer_cls: !!python/name:torch.optim.adam.Adam ''
optimizer_kwargs:
  lr: 1.0e-05
output_chunk_length: 256
output_dim: 1
pooling_kernel_sizes: !!python/tuple
- !!python/tuple
  - 256
- !!python/tuple
  - 16
- !!python/tuple
  - 1
train_sample_shape:
- !!python/tuple
  - 512
  - 1
- null
- null
- !!python/tuple
  - 256
  - 1
use_reversible_instance_norm: false
